---
title: "ReporteDistribuciones"
author: "Diana Laura Reyes Youshimatz 173391"
date: "2023-01-24"
output:
  html_document:
    df_print: paged
  word_document: default
  pdf_document: default
---

# Distribución Binomial
La distribución binomial es una distribución de probabilidad discreta que cuenta el número de éxitos en una
secuencia de $n$ ensayos de Bernoulli independientes entre sí, con una probabilidad fija p de ocurrencia del
éxito entre los ensayos. Para $n = 1$ , la binomial se convierte, de hecho, en una distribución de _Bernoulli._

\[p(x)= {n\choose x}p^x(1-p)^{n-x}\quad x=\{0,1,2,...,n\}\]

## Distribucion binomial con $n=10, p=0.1$
```{r}
# Semilla aleatoria
set.seed(999)
# Muestras binomiales de tamaño 10,50,100,1000
bin1<-list("N=10"=rbinom(10,10,0.1),"N=50"=rbinom(50,10,0.1),
           "N=100"=rbinom(100,10,0.1),"N=1000"=rbinom(1000,10,0.1))
# Clases del histogramas
b<-(-0.5:10.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
  hist(unlist(bin1[i]),freq=F,breaks=b,ylim=c(0,0.5),col='lightblue',xlab="X",
       ylab="Densidad",main=paste0("Binomial(10,0.1)\n",names(bin1)[i]))
  points(b+0.5,dbinom(b+0.5,10,0.1),col=2,pch=19)
  points(b+0.5,dbinom(b+0.5,10,0.1),col=2,type='h',lwd=2)
}
```
Se puede observar que para una distribución binomial con parámetro $p<0.5$ la distribución presenta
asimetría hacia la izquierda. Por otra parte la distribución empírica se ajusta a la distribución teórica para
tamaños de muestra grandes; para el caso de la distribución con parámetro $p=0.1$, se observa que hay un
buen ajuste para tamaños de muestra $N\geq50$.

## Distribución Binomial con $n=10, p=0.5$

```{r}
# Semilla aleatoria
set.seed(999)
# Muestras binomiales de tamaño 10,50,100,1000
bin1<-list("N=10"=rbinom(10,10,0.5),"N=50"=rbinom(50,10,0.5),
"N=100"=rbinom(100,10,0.5),"N=1000"=rbinom(1000,10,0.5))
# Clases del histogramas
b<-(-0.5:10.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(bin1[i]),freq=F,breaks=b,ylim=c(0,0.3),col='lightblue',xlab="X",
ylab="Densidad",main=paste0("Binomial(10,0.5)\n",names(bin1)[i]))
points(b+0.5,dbinom(b+0.5,10,0.5),col=2,pch=19)
points(b+0.5,dbinom(b+0.5,10,0.5),col=2,type='h',lwd=2)
}
```

Se puede observar que para una distribución binomial con parámetro $p=0.05$ la distribución teórica es
simétrica. Por otra parte la distribución empírica se ajusta a la distribución teórica para tamaños de muestra muy grandes; contrario al caso anterior, el ajuste es más lento y se require tamaños de muestra $N\geq1000$.

## Distribución Binomial con $n=10, p=0.9$

```{r}
# Semilla aleatoria
set.seed(999)
# Muestras binomiales de tamaño 10,50,100,1000
bin1<-list("N=10"=rbinom(10,10,0.9),"N=50"=rbinom(50,10,0.9),
"N=100"=rbinom(100,10,0.9),"N=1000"=rbinom(1000,10,0.9))
# Clases del histogramas
b<-(-0.5:10.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(bin1[i]),freq=F,breaks=b,ylim=c(0,0.5),col='lightblue',xlab="X",
ylab="Densidad",main=paste0("Binomial(10,0.9)\n",names(bin1)[i]))
points(b+0.5,dbinom(b+0.5,10,0.9),col=2,pch=19)
points(b+0.5,dbinom(b+0.5,10,0.9),col=2,type='h',lwd=2)
}
```

Se puede observar que para una distribución binomial con parámetro $p>0.5$ la distribución presenta
asimetría hacia la derecha. Por otra parte la distribución empírica se ajusta a la distribución teórica para tamaños de muestra grandes; para el caso de la distribución con parámetro $p=0.9$, se observa que hay unbuen ajuste para tamaños de muestra $N\geq50$.

# Distribución Geométrica
La variable aleatoria geométrica representa el número de fallas que ocurren antes de que se presente el primeréxito. La función de masa de probabilidad está dada por:
\[p(x)=(1-p)^{x-1}p\]

## Distribución Geometrica con $p=0.3$

```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 10, 50, 100 y 1000
geo<-list("N=10"=rgeom(10,0.3),"N=50"=rgeom(50,0.3),
"N=100"=rgeom(100,0.3),"N=1000"=rgeom(1000,0.3))
# Clases del histograma
b<-seq(-0.5,max(geo[[4]])+0.5)
# Múltiples Gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(geo[i]),freq=F,breaks=b,ylim=c(0,0.3),col='lightgreen',xlab="X",
ylab="Densidad",main=paste0("Geométrica(0.3)\n",names(geo)[i]))
points(b+0.5,dgeom(b+0.5,0.3),col=2,pch=19,cex=0.8)
points(b+0.5,dgeom(b+0.5,0.3),col=2,type='h',lwd=2)
}
```
Se observa que la distribución teórica es asimétrica hacia la izquierda, pues cuenta el número de ensayos antes de obtener el primer éxito, por otra parte el número de ensayos aumenta cuando el parámetro p es pequeño, además la distribución empírica se ajusta a la teórica para tamaños de muestra grandes; en este caso la distribución con $p=0.3$, el ajuste de la distribución se alcanza para tamaños de muestra $N\geq1000$.

## Distribución Geometrica con $p=0.5$

```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 10, 50, 100 y 1000
geo<-list("N=10"=rgeom(10,0.5),"N=50"=rgeom(50,0.5),
"N=100"=rgeom(100,0.5),"N=1000"=rgeom(1000,0.5))
# Clases del histograma
b<-seq(-0.5,max(geo[[4]])+0.5)
# Múltiples Gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(geo[i]),freq=F,breaks=b,ylim=c(0,0.6),col='lightgreen',xlab="X",
ylab="Densidad",main=paste0("Geométrica(0.5)\n",names(geo)[i]))
points(b+0.5,dgeom(b+0.5,0.5),col=2,pch=19)
points(b+0.5,dgeom(b+0.5,0.5),col=2,type='h',lwd=2)
}
```

Se observa que la distribución teórica es asimétrica hacia la izquierda, pues cuenta el número de ensayos antes de obtener el primer éxito, por otra parte el número de ensayos aumenta cuando el parámetro p es pequeño, además la distribución empírica se ajusta a la teórica para tamaños de muestra grandes; en este caso la distribución con $p=0.5$, el ajuste de la distribución se alcanza para tamaños de muestra $N\geq100$

## Distribución Geometrica con $p=0.7$
```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 10, 50, 100 y 1000
geo<-list("N=10"=rgeom(10,0.7),"N=50"=rgeom(50,0.7),
"N=100"=rgeom(100,0.7),"N=1000"=rgeom(1000,0.7))
# Clases del histograma
b<-seq(-0.5,max(geo[[4]])+0.5)
# Múltiples Gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(geo[i]),freq=F,breaks=b,ylim=c(0,0.8),col='lightgreen',xlab="X",
ylab="Densidad",main=paste0("Geométrica(0.7)\n",names(geo)[i]))
points(b+0.5,dgeom(b+0.5,0.7),col=2,pch=19)
points(b+0.5,dgeom(b+0.5,0.7),col=2,type='h',lwd=2)
}
```
Se observa que la distribución teórica es asimétrica hacia la izquierda, pues cuenta el número de ensayos antes de obtener el primer éxito, por otra parte el número de ensayos disminuye cuando el parámetro p esgrande, además la distribución empírica se ajusta a la teórica para tamaños de muestra grandes; en este caso la distribución con $p=0.7$, el ajuste de la distribución se alcanza para tamaños de muestra $N\geq50$

# Distribución Binomial Negativa

Esta distribución puede considerarse como una extensión o ampliación de la distribución geométrica. La distribución binomial negativa es un modelo adecuado para tratar aquellos procesos en los que se repite un determinado ensayo o prueba hasta conseguir un número determinado de resultados favorables (por vez
primera) .Es por tanto de gran utilidad para aquellos muestreos que procedan de esta manera. Si el número de resultados favorables buscados fuera 1 estaríamos en el caso de la distribución geométrica.

\[p(x)= {x-1\choose x-k}p^x(1-p)^{x-k}={x-1\choose x-k}p^k(1-p)^{x-k}\] 

## Distribución Binomial Negativa con $k=3 \quad y\quad p=0.3$

```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 10, 50, 100 y 1000
nbin<-list("N=10"=rnbinom(10,3,0.3),"N=50"=rnbinom(50,3,0.3),
"N=100"=rnbinom(100,3,0.3),"N=1000"=rnbinom(1000,3,0.3))
# Clases del histograma
b<-seq(-0.5,max(nbin[[4]])+0.5)
# Múltiples Gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(nbin[i]),freq=F,breaks=b,ylim=c(0,0.15),col='moccasin',xlab="X",
ylab="Densidad",main=paste0("Nbinomial(3,0.3)\n",names(nbin)[i]))
points(b+0.5,dnbinom(b+0.5,3,0.3),col=2,pch=19,cex=0.8)
points(b+0.5,dnbinom(b+0.5,3,0.3),col=2,type='h',lwd=1)
}
```

Se observa que la distribución empírica se ajusta a la teórica con tamaños de muestra grandes. En particular para un número de éxitos fijo $k=3$, se observa que la distribución es asimétrica hacia la izquierda y con $p=0.3$ la distribución empírica se ajusta con tamaños de muestra $N\geq1000$. Por otra parte, a medida que aumenta el parámetro $p$ el número de ensayos disminuyen y la densidad de probabilidad aumenta.

## Distribución Binomial Negativa con $k=3 \quad y\quad p=0.5$

```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 10, 50, 100 y 1000
nbin<-list("N=10"=rnbinom(10,3,0.5),"N=50"=rnbinom(50,3,0.5),
"N=100"=rnbinom(100,3,0.5),"N=1000"=rnbinom(1000,3,0.5))
# Clases del histograma
b<-seq(-0.5,max(nbin[[4]])+0.5)
# Múltiples Gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(nbin[i]),freq=F,breaks=b,ylim=c(0,0.3),col='moccasin',xlab="X",
ylab="Densidad",main=paste0("Nbinomial(3,0.5)\n",names(nbin)[i]))
points(b+0.5,dnbinom(b+0.5,3,0.5),col=2,pch=19)
points(b+0.5,dnbinom(b+0.5,3,0.5),col=2,type='h',lwd=2)
}
```

Se observa que la distribución empírica se ajusta a la teórica con tamaños de muestra grandes. En particular para un número de éxitos fijo $k=3$, se observa que la distribución es asimétrica hacia la izquierda y con $p=0.5$ la distribución empírica se ajusta con tamaños de muestra $N\geq1000$. Por otra parte, a medida que aumenta el parámetro $p$ el número de ensayos disminuye y la densidad de probabilidad aumenta.

## Distribución Binomial Negativa con $k=3 \quad y \quad p=0.7$

```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 10, 50, 100 y 1000
nbin<-list("N=10"=rnbinom(10,3,0.7),"N=50"=rnbinom(50,3,0.7),
"N=100"=rnbinom(100,3,0.7),"N=1000"=rnbinom(1000,3,0.7))
# Clases del histograma
b<-seq(-0.5,max(nbin[[4]])+0.5)
# Múltiples Gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(nbin[i]),freq=F,breaks=b,ylim=c(0,0.5),col='moccasin',xlab="X",
ylab="Densidad",main=paste0("Nbinomial(3,0.7)\n",names(nbin)[i]))
points(b+0.5,dnbinom(b+0.5,3,0.7),col=2,pch=19)
points(b+0.5,dnbinom(b+0.5,3,0.7),col=2,type='h',lwd=2)
}
```

Se observa que la distribución empírica se ajusta a la teórica con tamaños de muestra grandes. En particular para un número de éxitos fijo $k=3$, se observa que la distribución es asimétrica hacia la izquierda y con $p=0.7$ la distribución empírica se ajusta con tamaños de muestra $N\geq1000$. Por otra parte, a medida que aumenta el parámetro $p$ el número de ensayos disminuyen y la densidad de probabilidad aumenta.

## Distribución Binomial Negativa con $k=30 \quad y \quad p=[0.3,0.5,0.7]$

```{r}
# Semilla Aleatoria
set.seed(999)
# Muestras Geométricas de tamaño 1000
nbin<-list("N=1000"=rnbinom(1000,30,0.3),"N=1000"=rnbinom(1000,30,0.5),
"N=1000"=rnbinom(1000,30,0.7))
# Clases del histograma
b<-list(seq(-0.5,max(nbin[[1]])+0.5),seq(-0.5,max(nbin[[2]])+0.5),
seq(-0.5,max(nbin[[3]])+0.5))
p<-c(0.3,0.5,0.7)
# Múltiples Gráficos
par(mfrow=c(1,3))
for(i in 1:3)
{
hist(unlist(nbin[i]),freq=F,breaks=b[[i]],col='moccasin',xlab="X",
ylab="Densidad",main=paste0("Nbinomial(30,",p[i],")\n",names(nbin)[i]))
points(b[[i]]+0.5,dnbinom(b[[i]]+0.5,30,p[i]),col=2,pch=19,cex=0.7)
points(b[[i]]+0.5,dnbinom(b[[i]]+0.5,30,p[i]),col=2,type='h',lwd=1)
}
```

Se observa que la distribución empírica se ajusta a la teórica con tamaños de muestra grandes. Se puede observar que a medida que el parámetro $p$ aumenta, disminuye el número de ensayos antes de obtener los $k$ éxitos y aumenta la densidad de probabilidad; por otra parte cuando el parámetro $k$ aumenta se observa la
distribución tiende a ser simétrica.

# Distribución Hipergeométrica
La distribución hipergeométrica es una distribución discreta que modela el número de eventos en una muestra
de tamaño fijo cuando se conoce el número total de elementos en la población de la cual proviene la muestra.
Las muestras no tienen reemplazo, por lo que cada elemento de la muestra es diferente. Cuando se elige un elemento de la población, no se puede volver a elegir. Por lo tanto, la probabilidad de que un elemento en particular sea seleccionado aumenta con cada ensayo, suponiendo que aún no ha sido seleccionado.

\[ P(X=x)={{{{k}\choose{x}}{{N-k}\choose{n-x}}}\over{{N}\choose{n}}}\quad x=0,1,...,k \]

Donde $N$ es el tamaño de población, $n$ es el tamaño de la muestra extraída, $k$ es el número de elementos en
la población original que pertenecen a la categoría deseada y $x$ es el número de elementos en la muestra que pertenecen a dicha categoría.

## Distribución hipergeométrica $N=80 \quad k=10 \quad n=10$
```{r}
# Semilla aleatoria
set.seed(999)
# Muestras Hipergeométricas de tamaño 30,100,300,1000
N<-80; k<-10; n<-10
hyp<-list("Tamaño=30"=rhyper(30,n,N-n,k),"Tamaño=100"=rhyper(100,n,N-n,k),
"Tamaño=300"=rhyper(300,n,N-n,k),"Tamaño=1000"=rhyper(1000,n,N-n,k))
# Clases del histogramas
b<-seq(-0.5,max(hyp[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(hyp[i]),freq=F,breaks=b,ylim=c(0,0.45),col='lightblue',xlab="X",
main=paste0("Hipergeométrica k = ",k,' N = ',N,' n = ',n,'\n',names(hyp)[i]),
cex.main=0.8,ylab="Densidad")
points(b+0.5,dhyper(b+0.5,n,N-n,k),col=2,pch=19,cex=0.8)
points(b+0.5,dhyper(b+0.5,n,N-n,k),col=2,type='h',lwd=2)
}
```

En el gráfico se ve que cuando el parámetro que denota el número de elementos de interés en la población original ($k$) es pequeño e igual a la muestra extraída ($n$), la distribución es asimétrica hacia la derecha, además
a medida que el tamaño de muestra aumenta la distribución muestral se ajusta a la función de densidad con
tamaños superiores a $100$.

## Distribución hipergeométrica $N=60 \quad k=10  \quad n=25$

```{r}
# Semilla aleatoria
set.seed(999)
# Muestras Hipergeométricas de tamaño 30,100,300,1000
N<-60; k<-10; n<-25
hyp<-list("Tamaño=30"=rhyper(30,n,N-n,k),"Tamaño=100"=rhyper(100,n,N-n,k),
"Tamaño=300"=rhyper(300,n,N-n,k),"Tamaño=1000"=rhyper(1000,n,N-n,k))
# Clases del histogramas
b<-seq(-0.5,max(hyp[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(hyp[i]),freq=F,breaks=b,ylim=c(0,0.35),col='lightblue',xlab="X",
main=paste0("Hipergeométrica k = ",k,' N = ',N,' n = ',n,'\n',names(hyp)[i]),
cex.main=.8,ylab="Densidad")
points(b+0.5,dhyper(b+0.5,n,N-n,k),col=2,pch=19,cex=0.8)
points(b+0.5,dhyper(b+0.5,n,N-n,k),col=2,type='h',lwd=2)
}
```
En el gráfico se ve que cuando el parámetro que denota el número de elementos de interés en la población original ($k$) es menor a la muestra extraída ($n$), la distribución tiende a ser simétrica, además a medida que el
tamaño de muestra aumenta la distribución muestral se ajusta a la función de densidad con tamaños
superiores a $300$.

## Distribución hipergeométrica $N=60 \quad k=35 \quad n=25$
```{r}
# Semilla aleatoria
set.seed(999)
# Muestras Hipergeométricas de tamaño 30,100,300,1000
N<-60; k<-35; n<-25
hyp<-list("Tamaño=30"=rhyper(30,n,N-n,k),"Tamaño=100"=rhyper(100,n,N-n,k),
"Tamaño=300"=rhyper(300,n,N-n,k),"Tamaño=1000"=rhyper(1000,n,N-n,k))
# Clases del histogramas
b<-seq(-0.5,max(hyp[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(hyp[i]),freq=F,breaks=b,ylim=c(0,0.25),col='lightblue',xlab="X",
main=paste0("Hipergeométrica k = ",k,' N = ',N,' n = ',n,'\n',names(hyp)[i]),
cex.main=.8,ylab="Densidad")
points(b+0.5,dhyper(b+0.5,n,N-n,k),col=2,pch=19,cex=0.5)
points(b+0.5,dhyper(b+0.5,n,N-n,k),col=2,type='h',lwd=1)
}
```

En el gráfico se ve que cuando el parámetro que denota el número de elementos de interés en la población original ($k$) es mayor a la muestra extraída ($n$), la distribución es asimétrica hacia la derecha, además a medida que el tamaño de muestra aumenta la distribución muestral se ajusta a la función de densidad con tamaños superiores a $100$.

# Distribución Poisson 

La distribución de _Poisson_ es una distribución de probabilidad discreta que expresa, a partir de una frecuencia de ocurrencia media, la probabilidad de que ocurra un determinado número de eventos durante cierto período de tiempo. Concretamente, se especializa en la probabilidad de ocurrencia de sucesos con probabilidades muy
pequeñas, o sucesos “raros”.

\[p(x;\lambda)={{e^{-\lambda}\lambda^x}\over{x!}}\]

## Distribución Poisson $\lambda = 2$
```{r}
# Semilla aleatoria
set.seed(999)
# Muestras poisson de tamaño 30,100,300,1000
pois<-list("N=30"=rpois(30,2),"N=100"=rpois(100,2),
"N=300"=rpois(300,2),"N=1000"=rpois(1000,2))
# Clases del histogramas
b<-seq(-0.5,max(pois[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(pois[i]),freq=F,breaks=b,ylim=c(0,0.3),col='lemonchiffon',xlab="X",
ylab="Densidad",main=paste0("Poisson (2)\n",names(pois)[i]))
points(b+0.5,dpois(b+0.5,2),col=2,pch=19)
points(b+0.5,dpois(b+0.5,2),col=2,type='h',lwd=2)
}
```

En el gráfico se evidencia que la distribución _Poisson_ es asimétrica hacia la izquerda para valores del parámetro $\lambda$ pequeños, además la distribución muestral se ajusta a la distribución teórica para tamaños de muestra $N\geq300$.

## Distribución Poisson $\lambda = 5$

```{r}
# Semilla aleatoria
set.seed(999)
# Muestras poisson de tamaño 30,100,300,1000
pois<-list("N=30"=rpois(30,5),"N=100"=rpois(100,5),
"N=300"=rpois(300,5),"N=1000"=rpois(1000,5))
# Clases del histogramas
b<-seq(-0.5,max(pois[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(pois[i]),freq=F,breaks=b,ylim=c(0,0.20),col='lemonchiffon',xlab="X",
ylab="Densidad",main=paste0("Poisson (5)\n",names(pois)[i]))
points(b+0.5,dpois(b+0.5,5),col=2,pch=19)
points(b+0.5,dpois(b+0.5,5),col=2,type='h',lwd=2)
}
```

En el gráfico se evidencia que la distribución Poisson es asimétrica hacia la izquierda para valores del
parámetro $\lambda$ pequeños, sin embargo, a medida que aumenta el valor del parámetro, la distribución presenta cola hacia la izquierda, además la distribución muestral se ajusta a la distribución teórica para tamaños de muestra $N\geq300$ .

## Distribución Poisson $\lambda = 5$
```{r}
# Semilla aleatoria
set.seed(999)
# Muestras poisson de tamaño 30,100,300,1000
pois<-list("N=30"=rpois(30,5),"N=100"=rpois(100,5),
"N=300"=rpois(300,5),"N=1000"=rpois(1000,5))
# Clases del histogramas
b<-seq(-0.5,max(pois[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(pois[i]),freq=F,breaks=b,ylim=c(0,0.20),col='lemonchiffon',xlab="X",
ylab="Densidad",main=paste0("Poisson (5)\n",names(pois)[i]))
points(b+0.5,dpois(b+0.5,5),col=2,pch=19)
points(b+0.5,dpois(b+0.5,5),col=2,type='h',lwd=2)
}
```
En el gráfico se evidencia que la distribución Poisson es asimétrica hacia la izquierda para valores del
parámetro $\lambda$ pequeños, sin embargo, a medida que aumenta el valor del parámetro, la distribución presenta cola hacia la izquierda, además la distribución muestral se ajusta a la distribución teórica para tamaños de muestra $N\geq300$.

## Distribución Poisson $\lambda = 10$

```{r}
# Semilla aleatoria
set.seed(999)
# Muestras poisson de tamaño 30,100,300,1000
pois<-list("N=30"=rpois(30,10),"N=100"=rpois(100,10),
"N=300"=rpois(300,10),"N=1000"=rpois(1000,10))
# Clases del histogramas
b<-seq(-0.5,max(pois[[4]])+0.5)
# Múltiples gráficos
par(mfrow=c(2,2))
for(i in 1:4)
{
hist(unlist(pois[i]),freq=F,breaks=b,ylim=c(0,0.15),col='lemonchiffon',xlab="X",
ylab="Densidad",main=paste0("Poisson (10)\n",names(pois)[i]))
points(b+0.5,dpois(b+0.5,10),col=2,pch=19,cex=0.5)
points(b+0.5,dpois(b+0.5,10),col=2,type='h',lwd=1)
}
```
En el gráfico se evidencia que para valores grandes del parámetro $\lambda$ la distribución _Poisson_ se hace simétrica
presentando colas hacia ambos lados de la distribución, además la distribución muestral se ajusta a la distribución teórica para tamaños de muestra $N\geq300$.

# Distribución multinomial

La distribución multinomial es una generalización de la distribución binomial, la cual es la probabilidad de un número de éxitos en $N$ sucesos de Bernoulli independientes, con la misma probabilidad de éxito en cada suceso. En una distribución multinomial, el análogo a la distribución de Bernoulli es la distribución categórica, donde cada suceso concluye en únicamente un resultado de un número finito $K$ de los posibles, con probabilidades $p_1,...,p_k$ (tal que $p_i\geq0$ para $i$ entre $1$ y $K$ y $\sum_{i=1}^kp_i=1$) y con $n$ sucesos independientes.

Entonces sea la variable aleatoria $x_i$, que indica el número de veces que se ha dado el resultado $i$ sobre los $n$ sucesos. El vector $x=(x_1,...,x_k)$ sigue una distribución multinomial con parámetros $n$ y $p$, donde $p=(p_1,...,p_k)$ la distribución multinomial es una generalización de la distribución binomial.
Para el caso de la _distribución trinomial_, la función de masa de probabilidad es:

\[p(x,y)={n!\over{x!y!(n-x-y)!}}p_1^x p_2^y(1-p_1-p_2)^{n-x-y}, \quad cuando\quad x+y\leq n\]

## Distribución Trinomial $p_1=0.1\quad p_2=0.8 \quad n=10$

```{r}
# Semilla aleatoria
library("plot3D")
set.seed(999)
# Vector de probablidades
p<-c(0.1,0.8);p[3]<-1-sum(p)
# Número de ensayos
n<-10
# Muestra aleatoria multinomial de tamaño 1000
x<-t(rmultinom(10000,n,p))
# Matriz de densidades
d<-table(x[,1],x[,2])/100
# Histograma 3D
par(mar=c(2,2,2,2))
hist3D(z=d,col='aquamarine',border=1,phi = 30,theta=-25,d=2,bty='g',
       shade = .35, zlab = "Densidad",space = 0.1,cex.axis = 1e-9)
```

Por la ley de los grandes números se sabe que a medida que el tamaño de muestra aumenta la distribución
empírica se ajusta a la distribución teórica. Para la distribución trinomial se evidencia que a medida que la
probabilidad $p_2$ aumenta, la densidad se recarga en valores altos para el eje $y$ y en valores bajos para el eje $x$.

## Distribución Trinomial $p_1=0.45\quad p_2=0.45 \quad n=10$

```{r}
# Semilla aleatoria
set.seed(999)
# Vector de probablidades
p<-c(0.45,0.45);p[3]<-1-sum(p)
# Número de ensayos
n<-10
# Muestra aleatoria multinomial de tamaño 1000
x<-t(rmultinom(10000,n,p))
# Matriz de densidades
d<-table(x[,1],x[,2])/100
# Histograma 3D
par(mar=c(2,2,2,2))
hist3D(z=d,col='aquamarine',border=1,phi = 30,theta=-25,d=2,bty='g',
       shade = .35, zlab = "Densidad",space = 0.1,cex.axis = 1e-9)

```

Por la ley de los grandes números se sabe que a medida que el tamaño de muestra aumenta la distribución
empírica se ajusta a la distribución teórica. Para la distribución trinomial se evidencia que a medida que cuando
la probabilidad $p_2=p_1$, la densidad se recarga en el centro del plano $(x,y)$.

## Distribución Trinomial $p_1=0.8\quad p_2=0.1 \quad n=10$

```{r}
# Semilla aleatoria
set.seed(999)
# Vector de probablidades
p<-c(0.8,0.1);p[3]<-1-sum(p)
# Número de ensayos
n<-10
# Muestra aleatoria multinomial de tamaño 1000
x<-t(rmultinom(10000,n,p))
# Matriz de densidades
d<-table(x[,1],x[,2])/100
# Histograma 3D
par(mar=c(2,2,2,2))
hist3D(z=d,col='aquamarine',border=1,phi = 30,theta=-25,d=2,bty='g',
       shade = .35, zlab = "Densidad",space = 0.1,cex.axis = 1e-9)
```

Por la ley de los grandes números se sabe que a medida que el tamaño de muestra aumenta la distribución
empírica se ajusta a la distribución teórica. Para la distribución trinomial se evidencia que a medida que la probabilidad $p1$ aumenta, la densidad se recarga en valores altos para el eje $x$ y en valores bajos para el eje $y$.

## Distribución $\chi^2$
La distribución de _Pearson_, llamada también ji cuadrada(o) o chi cuadrado(a) ($\chi^2$), es una distribución de
probabilidad continua con un parámetro $r$ que representa los grados de libertad de la variable aleatoria.

\[  f(x) =
    \begin{cases}
      {1\over{2^{r/2}\Gamma(r/2)}}x^{(r/2)-1}e^{-x/2} & \text{para $x>0$}\\
      0 & \text{en otro caso}
    \end{cases}    
  \]
  
```{r}
curve(dchisq(x,2),lwd=2,ylab='Densidad',,xlim=c(0,25),ylim=c(0,0.3),col='salmon',
main = expression('Distribución '*chi[(r)]^2))
curve(dchisq(x,3),col=1,lwd=2,add=T)
curve(dchisq(x,5),col=2,lwd=2,add=T)
curve(dchisq(x,7),col=3,lwd=2,add=T)
curve(dchisq(x,9),col=4,lwd=2,add=T)
curve(dchisq(x,11),col=6,lwd=2,add=T)
legend('topright',legend=c('r = 2','r = 3','r = 5','r = 7','r = 9','r = 11'),
col=c('salmon',1:4,6),lty=1,lwd=2,title='Grados de Libertad')
```

# Distribución Gamma 

Este modelo es una generalización del modelo Exponencial ya que, en ocasiones, se utiliza para modelar
variables que describen el tiempo hasta que se produce $\alpha$ veces un determinado suceso.

\[  f(x) =
    \begin{cases}
      {1\over{\Gamma(\alpha)\beta^{{\alpha}}}}x^{a-1}e^{-x\over\beta} & \text{para $x>0, \alpha>0, \beta>0$}\\
      0 & \text{en otro caso}
    \end{cases}    
  \]
```{r}
par(mfrow=c(2,3))
curve(dgamma(x,shape=2,scale=1/4),xlim=c(0,5),lwd=2,ylab='Densidad',
      main=expression("Distribución Gamma "*alpha==2))
curve(dgamma(x,shape = 2,scale = 1/2),lty=2,lwd=2,add=T)
curve(dgamma(x,shape = 2,scale = 1),lty=4,lwd=2,add=T)
legend('topright',col=1,lty=c(1,2,4),lwd=2,legend=c(expression(beta==1/4),
                                                    expression(beta==1/2),
                                                    expression(beta==1)))
curve(dgamma(x,shape=3,scale=1/4),xlim=c(0,5),col=2,lwd=2,ylab='Densidad',
      main=expression("Distribución Gamma "*alpha==3))
curve(dgamma(x,shape = 3,scale = 1/2),lty=2,col=2,lwd=2,add=T)
curve(dgamma(x,shape = 3,scale = 1),lty=4,col=2,lwd=2,add=T)
legend('topright',col=2,lty=c(1,2,4),lwd=2,legend=c(expression(beta==1/4),
                                                    expression(beta==1/2),
                                                    expression(beta==1)))
curve(dgamma(x,shape = 4,scale = 1/4),xlim=c(0,6),col=4,lwd=2,ylab='Densidad',
      main=expression("Distribución Gamma "*alpha==4))
curve(dgamma(x,shape = 4,scale = 1/2),lty=2,col=4,lwd=2,add=T)
curve(dgamma(x,shape = 4,scale = 1),lty=4,col=4,lwd=2,add=T)
legend('topright',col=4,lty=c(1,2,4),lwd=2,legend=c(expression(beta==1/4),
                                                    expression(beta==1/2),
                                                    expression(beta==1)))
curve(dgamma(x,shape = 2,scale = 1/4),xlim=c(0,4),lwd=2,ylab='Densidad',
      main=expression("Distribución Gamma "*beta==1/4))
curve(dgamma(x,shape = 3,scale = 1/4),col=2,lwd=2,add=T)
curve(dgamma(x,shape = 4,scale = 1/4),col=4,lwd=2,add=T)
legend('topright',col=c(1,2,4),lty=1,lwd=2,legend=c(expression(alpha==2),
                                                    expression(alpha==3),
                                                    expression(alpha==4)))
curve(dgamma(x,shape = 2,scale = 1/2),xlim=c(0,5),lwd=2,lty=2,ylab='Densidad',
      main=expression("Distribución Gamma "*beta==1/2))
curve(dgamma(x,shape = 3,scale = 1/2),col=2,lwd=2,lty=2,add=T)
curve(dgamma(x,shape = 4,scale = 1/2),col=4,lwd=2,lty=2,add=T)
legend('topright',col=c(1,2,4),lty=2,lwd=2,legend=c(expression(alpha==2),
                                                    expression(alpha==3),
                                                    expression(alpha==4)))
curve(dgamma(x,shape = 2,scale = 1),xlim=c(0,7),lwd=2,lty=4,ylab='Densidad',
      main=expression("Distribución Gamma "*beta==1))
curve(dgamma(x,shape = 3,scale = 1),col=2,lwd=2,lty=4,add=T)
curve(dgamma(x,shape = 4,scale = 1),col=4,lwd=2,lty=4,add=T)
legend('topright',col=c(1,2,4),lty=4,lwd=2,legend=c(expression(alpha==2),
                                                    expression(alpha==3),
                                                    expression(alpha==4)))
#
```
  
En el gráfico se evidencia como el parámetro $\alpha$ modifica la forma de la distribución, entre más pequeño sea este parámetro se acumula más rápido la probabilidad, además con distintos valores del parámetro de escala $\beta$ la distribución se dispersa sobre el eje $x$ disminuyendo la densidad.

# Distribución Normal

La _distribución normal_, _distribución de Gauss_ o _distribución gaussiana_, es una de las distribuciones de
probabilidad de variable continua que con más frecuencia aparece aproximada en fenómenos reales. La
importancia de esta distribución radica en que permite modelar numerosos fenómenos naturales, sociales y
psicológicos. Mientras que los mecanismos que subyacen a gran parte de este tipo de fenómenos son
desconocidos, por la enorme cantidad de variables incontrolables que en ellos intervienen, el uso del modelo
normal puede justificarse asumiendo que cada observación se obtiene como la suma de unas pocas causas
independientes.
\[f(x)={1\over{\sigma\sqrt{2\pi}}}e^{-(x-\mu)\over{2\sigma^2}}\quad -\infty<x<\infty\]
La gráfica de su función de densidad tiene una forma acampanada y es simétrica respecto a un determinado
parámetro de posición.

```{r}
par("mar")
mu<-c(0,2,4,-5);
curve(dnorm(x),xlim=c(-10,10),main=expression('Distribución Normal '*sigma==1),lwd=2,
ylab='Densidad')
curve(dnorm(x,2,1),col=2,lwd=2,add=T)
curve(dnorm(x,4,1),col=3,lwd=2,add=T)
curve(dnorm(x,-5,1),col=4,lwd=2,add=T)
for(i in 1:4){polygon(c(mu[i],mu[i]),c(0,dnorm(mu[i],mu[i],1)),border=i,lty=4,lwd=2)}
legend('toprigh',lty=1,col=1:4,lwd=2,legend=c(expression(mu==0),expression(mu==2),
expression(mu==4),expression(mu==-5)))
sig<-c(1,0.7,2);sig<-cbind(-sig,sig)
curve(dnorm(x),lwd=2,ylim=c(0,0.6),xlim = c(-5,5),ylab='Densidad',
main=expression('Distribución Normal '*mu==0))
curve(dnorm(x,0,sig[2,2]),col=2,lwd=2,add=T)
curve(dnorm(x,0,sig[3,2]),col=3,lwd=2,add=T)
for(i in 1:3){polygon(sig[i,],dnorm(sig[i,],0,sig[i,2]),border = i,lty=4,lwd=2)}
legend('toprigh',lty=1,col=1:3,lwd=2,legend=c(expression(sigma==1),expression(sigma==0.7),
expression(sigma==2)))
```


En la gráfica se evidencia claramente cómo los parámetros modifican las funciones, para el parámetro $\mu$ la función se desplaza sobre el eje $x$ cambiando su posición. Por otra parte para el parámetro $\sigma$ cambia la escala de la distribución, a medida que aumenta $\sigma$ se achata más la función.


# Distribución normal Bivariada

Una _distribución normal multivariante_, también llamada _distribución gaussiana multivariante_, es una generalización de la distribución normal unidimensional a dimensiones superiores.
Para el caso bivariado el vector aleatorio $(X,Y)$, que toma todos los valores en el plano euclidiano, tiene la siguiente función de distribución de probabilidad:

\[f(x,y)={1\over{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}}\exp\{{-1\over{2(1-\rho^2)}}\}\]

\[f(x,y)= {1\over{2\pi\sigma_x\sigma_y\sqrt{1-\rho^2}}} \exp \left\{- \frac{1}{2(1-\rho^2)}\left[\left(\frac{x-\mu_x}{\sigma_z}\right)^2-2\rho\frac{(x-\mu_x)(y-\mu_y)}{\sigma_x\sigma_y} + \left(\frac{y-\mu_y}{\sigma_y}^2\right)\right]\right\}\]

## Distribución Normal Bivariada $\mu=[0,0]\quad\sigma=[5,5]\quad cov=0 \rho=0$

```{r}
x<-seq(-6,6,length=40);y<-x
# Función Normal Bivariada
f<-function(x,y,mu=c(0,0),s=c(1,0,1))
{
mu1<-mu[1];mu2<-mu[2]
s11<-s[1];s12<-s[2];s22<-s[3];rho<-s12/(s11*s22)
term1<-1/(2*pi*sqrt(s11*s22*(1-rho^2)))
term2<--1/(2*(1-rho^2))
term3<-(x-mu1)^2/s11
term4<-(y-mu2)^2/s22
term5<--2*rho*((x-mu1)*(y-mu2))/(sqrt(s11)*sqrt(s22))
term1*exp(term2*(term3+term4-term5))
}
z<-outer(x,y,f,s=c(5,0,5)) # matriz de densidades
persp(x, y, z,col="lightsalmon",theta=30, phi=20,r=50,d=0.1,expand=0.5,ltheta=90,
main="Distribución Normal Bivariada",lphi=180,shade=0.3,ticktype="detailed",
nticks=5,cex.axis=0.7,zlab = 'Densidad')
mtext(expression(list(mu[x]==0,mu[y]==0,sigma[x]==5,sigma[y]==5,
sigma[xy]==0,rho==0)), side=3)
```

En este gráfico se puede observar que la distribucíon normal bivariada cuando la correlación entre las variables $X$ y $Y$ es cero $(\rho=0)$, la superficie dibuja la clásica campana de gauss, en la posición marcada por el vector de medias de la distribución.

## Distribución Normal Bivariada $\mu=[0,0]\quad\sigma=[5,5]\quad cov=12.5 \rho=0.5$

```{r}
x<-seq(-6,6,length=40);y<-x
z<-outer(x,y,f,s=c(5,12.5,5)) # matriz de densidades
persp(x, y, z,col="lightsalmon",theta=30,phi=20,r=50,d=0.1,expand=0.5,ltheta=90,
main="Distribución Normal Bivariada",lphi=180,shade=0.3,ticktype="detailed",
nticks=5,cex.axis=0.7,zlab = 'Densidad')
mtext(expression(list(mu[x]==0,mu[y]==0,sigma[x]==5,sigma[y]==5,
sigma[xy]==12.5,rho==0.5)), side=3)
```

En este gráfico se puede observar que en la distribucíon normal bivariada cuando la correlación entre las variables es $rho=0.5$, la superficie dibuja la una cresta con un ángulo de 135° respecto al origen, es decir, la densidad de probabilidad aumenta a medida que los puntos críticos en $X$ aumentan y los $Y$ de disminuyen.

## Distribución Normal Bivariada $\mu=[0,0]\quad\sigma=[5,5]\quad cov=-12.5 \rho=-0.5$

```{r}
x<-seq(-6,6,length=40);y<-x
z<-outer(x,y,f,s=c(5,-12.5,5)) # matriz de densidades
persp(x, y, z,col="lightsalmon",theta=30, phi=20,r=50,d=0.1,expand=0.5,ltheta=90,
main="Distribución Normal Bivariada",lphi=180,shade=0.3,ticktype="detailed",
nticks=5,cex.axis=0.7,zlab = 'Densidad')
mtext(expression(list(mu[x]==0,mu[y]==0,sigma[x]==5,sigma[y]==5,
sigma[xy]==-12.5,rho==-0.5)), side=3)
```

En este gráfico se puede observar que en la distribucíon normal bivariada cuando la correlación entre las variables es $\rho=-0.5$, la superficie dibuja la una cresta con un ángulo de 90° respecto al origen, es decir, la densidad de probabilidad aumenta a medida que los puntos críticos en disminuyen y los de aumentan.

## Distribución Normal Bivariada $\mu=[0,0]\quad\sigma=[5,5]\quad cov=24 \rho=0.96$

```{r}
x<-seq(-6,6,length=40);y<-x
z<-outer(x,y,f,s=c(5,24,5)) # matriz de densidades
persp(x, y, z,col="lightsalmon",theta=30, phi=20,r=50,d=0.1,expand=0.5,ltheta=90,
main="Distribución Normal Bivariada",lphi=180,shade=0.3,ticktype="detailed",
nticks=5,cex.axis=0.7,zlab = 'Densidad')
mtext(expression(list(mu[x]==0,mu[y]==0,sigma[x]==5,sigma[y]==5,
sigma[xy]==24,rho==0.96)), side=3)
```

En este gráfico se puede observar que en la distribucíon normal bivariada cuando la correlación entre las variables es es cercana a 1, la cresta de la superfice es más pronunciada limitando a que las densidades exitan en los puntos críticos donde se cumple $X=Y$.

# Distribución t-student

La distribución _t-de Student_ es una distribución de probabilidad que surge del problema de estimar la media de una población normalmente distribuida cuando el tamaño de la muestra es pequeño. Su _f.d.p_ está dada por:

\[f(x)=\frac{\Gamma\left(\frac{\nu+1}{2}\right)}{\Gamma\left(\frac{\nu}{2}\right)\sqrt{\pi \nu}\sigma}\left[1+\frac{1}{v}\left(\frac{x-\mu}{\sigma}\right)^2\right]^{-\frac{\nu+1}{2}}\]

```{r}
curve(dnorm(x),xlim=c(-5,5),main='Distribución t-Student',ylab='Densidad',lwd=2)
curve(dt(x,df = 1),col=2,add=T,lwd=2)
curve(dt(x,df = 2),col=3,add=T,lwd=2)
curve(dt(x,df = 5),col=4,add=T,lwd=2)
legend('toprigh',lty=1,col=1:4,lwd=2,cex=0.9,
legend=c('Normal (0,1)','Cauchy','t(v=2)','t(v=5)'))
```

En esta gráfica podemos apreciar la realción que existe entre las distribuciones _Cauchy, t-de Student_ y la _Normal Estandarizada_. Se observa que la distribución t-de Student con grado de libertad es una distribución _Cauchy_ la cual es la más achatada y presenta las colas mas pesadas en comparación las demás distribuciones presentes en la gráfica. Además se puede evidenciar que a medida que aumentan los grados de libertad en la _distribución t_, ésta tiende a la _Normal_.

# Distribución Beta 
La distribución beta es una distribución de probabilidad continua con dos parámetros de forma $\alpha$ y $\beta$, utiles para estimar proporciones, ya que su función de densidad existe para valores $0\leq x \leq 1$ y está dada por:

\[f(x)=\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)} x^{(\alpha-1)}(1-x)^{(\beta-1)}\]

```{r}
par(mfrow=c(3,1))
curve(dbeta(x,1,1),xlim=c(0,1),ylim=c(0,3),ylab='Densidad',lwd=2,
main=expression('Distribución Beta '*alpha==beta))
curve(dbeta(x,2,2),col=2,lwd=2,add=T)
curve(dbeta(x,4,4),col=3,lwd=2,add=T)
curve(dbeta(x,7,7),col=4,lwd=2,add=T)
curve(dbeta(x,0.5,0.5),col=6,lwd=2,add=T)
legend(x=0.7,y=3,lty=1,col=c(1:4,6),lwd=2,
legend=c(expression(alpha*'='*beta==1),expression(alpha*'='*beta==2),
expression(alpha*'='*beta==4),expression(alpha*'='*beta==7),
expression(alpha*'='*beta==0.5)))
curve(dbeta(x,2,2),xlim=c(0,1),ylim=c(0,4),ylab='Densidad',lwd=2,
main=expression('Distribución Beta '*alpha<=beta))
curve(dbeta(x,2,4),col=2,lwd=2,add=T)
curve(dbeta(x,2,7),col=3,lwd=2,add=T)
curve(dbeta(x,2,9),col=4,lwd=2,add=T)
legend('toprigh',lty=1,col=1:4,lwd=2,title=expression(alpha==2),
legend=c(expression(beta==2),expression(beta==4),
expression(beta==7),expression(beta==9)))
curve(dbeta(x,2,2),xlim=c(0,1),ylim=c(0,4),ylab='Densidad',lwd=2,
main=expression('Distribución Beta '*alpha>=beta))
curve(dbeta(x,4,2),col=2,lwd=2,add=T)
curve(dbeta(x,7,2),col=3,lwd=2,add=T)
curve(dbeta(x,9,2),col=4,lwd=2,add=T)
legend('topleft',lty=1,col=1:4,lwd=2,title=expression(beta==2),
legend=c(expression(alpha==2),expression(alpha==4),
expression(alpha==7),expression(alpha==9)))
```

En esta distribución ambos parámetros $\alpha$ y $\beta$ determinan la forma de esta. Para el caso, cuando $\alpha=\beta$ se
observa que la distribución es simétrica; además se obtiene un resultado particular cuando $\alpha=\beta$ = 1 pues se
genera una distribución uniforme $(0,1)$. Cuando $\alpha<\beta$, la distribución comienza a tener un comportamiento asimétrico positivo el cual es más evidente a medida que $\beta$ aumenta. Finalmente, cuando la $\alpha>\beta$ distribución muestra un comportamiento asimétrico negativo el cual es más evidente a medida que $\alpha$ aumenta.

# Distribución _F_

La _distribución F_ es una distribución de probabilidad continua. También se le conoce como distribución F de Snedecor (por George Snedecor) o como distribución F de Fisher-Snedecor (por Ronald Fisher).
Una variable aleatoria de distribución F se construye como el siguiente cociente:

\[F=\frac{U_1/d_i}{U_2/d_2}\]

donde 
- $U_1$ y $U_2$ siguen una distribución $\chi^2$ con $d_1$ y $d_2$ grados de libertad respectivamente, y
- $U_1$ y $U_2$ son estadísticamente independientes 

La _distribución F_ aparece frecuentemente como la distribución nula de una prueba estadística, especialmente
en el análisis de varianza. Su función de densidad de probabilidad está dada por:

\[f(x;d_1,d_2)=\frac{\Gamma\left(\frac{d_1+d_2}{2}\right)}{\Gamma(\frac{d_1}{2})\Gamma(\frac{d_2}{2})}\left(\frac{d_1}{d_2}\right)^{\frac{d_1}{2}}x^{\frac{d_1}{2}-1}\left(1+\frac{d_1}{d_2}x\right)^{-\frac{d_1+d_2}{2}}; \quad x\geq0\]

```{r}
par(mfrow=c(1,2))
curve(df(x,3,4),xlim=c(0,3),ylim=c(0,0.8),lwd=2,ylab='Densidad',
main=expression('Distribución F '*d[1]==3))
curve(df(x,3,7),col=2,lwd=2,add=T)
curve(df(x,3,10),col=3,lwd=2,add=T)
curve(df(x,3,30),col=4,lwd=2,add=T)
legend('topright',lty=1,col=1:4,lwd=2,
legend=c(expression(d[2]==4),expression(d[2]==7),
expression(d[2]==10),expression(d[2]==30)))
curve(df(x,4,3),xlim=c(0,3),ylim=c(0,0.8),lwd=2,ylab='Densidad',
main=expression('Distribución F '*d[2]==3))
curve(df(x,7,3),col=2,lwd=2,add=T)
curve(df(x,10,3),col=3,lwd=2,add=T)
curve(df(x,30,3),col=4,lwd=2,add=T)
legend('topright',lty=1,col=1:4,lwd=2,
legend=c(expression(d[1]==4),expression(d[1]==7),
expression(d[1]==10),expression(d[1]==30)))
```

En el primer gráfico se evidencia cómo el parámetro $d_2$ determina la altura de la densidad cuando el parámetro $d_1$ está fijo, mientras que en el segundo se ve cómo el parámetro $d_1$ mueve la distribución en el eje $x$ de
forma positiva cuando el parámetro $d_2$ está fijo.

```{r}
par(mfrow=c(1,2))
curve(df(x,1,1),xlim=c(0,3),ylim=c(0,1),lwd=2,ylab='Densidad',
main=expression('Distribución F '*d[1]==d[2]))
curve(df(x,2,2),col=2,lwd=2,add=T)
curve(df(x,4,4),col=3,lwd=2,add=T)
curve(df(x,6,6),col=4,lwd=2,add=T)
curve(df(x,9,9),col=5,lwd=2,add=T)
legend('topright',lty=1,col=1:5,lwd=2,
legend=c(expression(d[1]*' = '*d[2]==1),expression(d[1]*' = '*d[2]==2),
expression(d[1]*' = '*d[2]==3),expression(d[1]*' = '*d[2]==5),
expression(d[1]*' = '*d[2]==7)))
curve(df(x,10,10),xlim=c(0,2),ylim=c(0,4),lwd=2,ylab='Densidad',
main=expression('Distribución F '*d[1]==d[2]))
curve(df(x,20,20),col=2,lwd=2,add=T)
curve(df(x,50,50),col=3,lwd=2,add=T)
curve(df(x,100,100),col=4,lwd=2,add=T)
curve(df(x,200,200),col=5,lwd=2,add=T)
curve(df(x,400,400),col=6,lwd=2,add=T)
legend('topright',lty=1,col=1:6,lwd=2,
legend=c(expression(d[1]*' = '*d[2]==10),expression(d[1]*' = '*d[2]==20),
expression(d[1]*' = '*d[2]==50),expression(d[1]*' = '*d[2]==100),
expression(d[1]*' = '*d[2]==200),expression(d[1]*' = '*d[2]==400)))
```

Cuando los parámetros de la distribución son iguales se nota que para valores $<3$ la distribución tiene tendencia exponencial, y aparece de forma notable la varianza con parámetros $d_1=d_2\geq4$. Por otra parte, si
ambos parámetros son muy grandes, en el límite la distribución se transforma en una distribución degenerada
en $x=1$.


